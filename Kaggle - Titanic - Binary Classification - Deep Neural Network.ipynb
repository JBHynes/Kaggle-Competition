{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T10:37:20.473044Z",
     "start_time": "2023-10-05T10:37:19.528744Z"
    },
    "execution": {
     "iopub.execute_input": "2023-10-05T09:55:23.892245Z",
     "iopub.status.busy": "2023-10-05T09:55:23.891873Z",
     "iopub.status.idle": "2023-10-05T09:55:25.041078Z",
     "shell.execute_reply": "2023-10-05T09:55:25.039565Z",
     "shell.execute_reply.started": "2023-10-05T09:55:23.892199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jhynes1/Documents/GitHub/Kaggle-Competition/titanic\r\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils_v3 import *\n",
    "from public_tests import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn\n",
    "import mpl_toolkits\n",
    "\n",
    " \n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "!pwd; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0) Deep Learning Neural Networks\n",
    "\n",
    "#### 5.1) Prepare data:\n",
    "    * Steps 0: prepare data inputs & network model parameters, ensure correct dimenssions\n",
    "\n",
    "#### 5.2) Write helper functions:\n",
    "    * Steps 1: parameters = def initialize parameters(layers_dim)\n",
    "    * Steps 2: AL, caches = L_model_forward(X, parameters):\n",
    "    * Steps 3: cost = compute_cost(AL, Y):\n",
    "    * Steps 4: grads = L_model_backward(AL, Y, caches):\n",
    "    * Steps 6: parameters =  update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "#### 5.3) Build model & beta test:\n",
    "    * Step 7: parameters, cost = L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 1, print_cost=False)\n",
    "    * Step 8: test :  print(\"Cost after first iteration: \" + str(costs[0]))\n",
    "\n",
    "#### 5.4) Train model:\n",
    "    * Step 9: parameters, costs = L_layer_model(train_x, train_y, layers_dims, num_iterations = 1000, print_cost = True)\n",
    "\n",
    "#### 5.5) Test model:\n",
    "    * Step 10: parameters, costs = L_layer_model(test_x, test_y, layers_dims, num_iterations = 1000, print_cost = True\n",
    "\n",
    "#### Generate prediction file:\n",
    "    * Step 11A: pred_target_y = L_Forward_model(test_x,paramters)\n",
    "        * Step 11B: threshold \n",
    "        * Step 11C: creat df and csv file. [passengerid predictions] \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "Features importance:\n",
      "68.85 Sex\n",
      "24.60 Pclass\n",
      "14.21 Fare\n",
      "3.13 Embarked\n",
      "1.83 Parch\n",
      "1.30 Age\n",
      "0.53 SibSp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"/Users/jhynes1/Documents/GitHub/Kaggle-Competition/titanic/train.csv\")\n",
    "test_df = pd.read_csv(\"/Users/jhynes1/Documents/GitHub/Kaggle-Competition/titanic/test.csv\")\n",
    "\n",
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "train_df['Embarked'].fillna(0, inplace=True)  # unkown \n",
    "train_df['Embarked'].replace('Q', 1,inplace=True)\n",
    "train_df['Embarked'].replace('S', 2,inplace=True)\n",
    "train_df['Embarked'].replace('C', 3,inplace=True)\n",
    "\n",
    "test_df['Embarked'].fillna(0, inplace=True)\n",
    "test_df['Embarked'].replace('Q', 1,inplace=True)\n",
    "test_df['Embarked'].replace('S', 2,inplace=True)\n",
    "test_df['Embarked'].replace('C', 3,inplace=True)\n",
    "\n",
    "train_df['Sex'].replace('male', 0,inplace=True)\n",
    "train_df['Sex'].replace('female', 1,inplace=True)\n",
    "\n",
    "test_df['Sex'].replace('male', 0,inplace=True)\n",
    "test_df['Sex'].replace('female', 1,inplace=True)\n",
    "\n",
    "#for data in combined_data:\n",
    "train_df.Fare.fillna(train_df.Fare.mean(), inplace = True)\n",
    "test_df.Fare.fillna(train_df.Fare.mean(), inplace = True)\n",
    "\n",
    "train_df.Age.fillna(method = 'ffill', inplace = True)\n",
    "test_df.Age.fillna(method='ffill', inplace = True)\n",
    "\n",
    "print(test_df.isnull().sum()) # inspect data types: any missing or null data \n",
    "print(train_df.isnull().sum()) # inspect data types: any missing or null data  \n",
    "\n",
    "## TEST PREDICITVE POWER \n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "columns_features_final = ['Sex', 'Pclass' ,'Embarked' ,  'Parch', 'SibSp', 'Age', 'Fare']\n",
    "\n",
    "selector = SelectKBest(f_classif, k='all')\n",
    "\n",
    "selector.fit(train_df[columns_features_final],train_df['Survived'])\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "indices = np.argsort(scores)[::-1]\n",
    "\n",
    "print('Features importance:')\n",
    "for i in range(len(scores)):\n",
    "    print('%.2f %s' % (scores[indices[i]], columns_features_final[indices[i]]))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Feature Engineering - Select final features and scale \n",
    "    \n",
    "columns_features_final = ['Sex', 'Pclass' ,'Embarked' , 'SibSp','Age'  ]\n",
    "\n",
    "train_df_features = train_df[['Survived'] + columns_features_final ]\n",
    "test_df_features = test_df[columns_features_final] \n",
    "\n",
    "# For submission scoring (i.e., don't normalize 'PassenderID' feature during subsequent feature engineering steps)\n",
    "test_df_features_Match = test_df[['PassengerId'] + columns_features_final ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler ()\n",
    "scaler.fit(train_df_features)\n",
    "scaled = scaler.fit_transform(train_df_features)\n",
    "train_df_features_norm = pd.DataFrame(scaled, columns=train_df_features. columns)\n",
    "\n",
    "scaler.fit(test_df_features)\n",
    "scaled = scaler.fit_transform(test_df_features)\n",
    "test_df_features_norm = pd.DataFrame(scaled, columns=test_df_features. columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 11)\n",
      "Total set size: 891\n",
      "Training set size: 713\n",
      "Validation set size: 178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(np.shape(test_df))\n",
    " ## SWAP OUT REG DATA FOR PCA ###\n",
    "#train_df_norm = pca_plot_df[['PC1', 'PC2','PC3','PC4', 'Survived']]\n",
    "\n",
    "#columns_to_be_added_as_features = ['PC1', 'PC2',  'PC3', 'PC4']\n",
    "#### SWAP OUT REG DATA FOR PCA  ###\n",
    "#train_df_features_norm = pca_plot_df\n",
    "\n",
    "\n",
    "#train_df_features_final = train_df_features_final.sample(frac=1).reset_index(drop=True)\n",
    " \n",
    "\n",
    "validation_set_ratio = 0.20 # 30%\n",
    "validation_set_size = int(len(train_df_features_norm)*validation_set_ratio)\n",
    "training_set_size = len(train_df_features_norm) - validation_set_size\n",
    "\n",
    "# test vs train = 20% split\n",
    "nn_train, nn_val = train_test_split(train_df_features_norm, test_size=validation_set_ratio)\n",
    "\n",
    "nn_train_x = nn_train[columns_features_final]\n",
    "nn_train_y = nn_train['Survived']\n",
    "\n",
    "nn_val_x = nn_val[columns_features_final]\n",
    "nn_val_y = nn_val['Survived'] \n",
    "\n",
    "print(\"Total set size: {}\".format(len(train_df_features_norm)))\n",
    "print(\"Training set size: {}\".format(training_set_size))\n",
    "print(\"Validation set size: {}\".format(validation_set_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 712) (1, 712)\n"
     ]
    }
   ],
   "source": [
    "train_x = nn_train_x.to_numpy().T\n",
    "train_y = nn_train_y.to_numpy()[np.newaxis,:]\n",
    "\n",
    "val_x = nn_val_x.to_numpy().T\n",
    "val_y = nn_val_y.to_numpy()[np.newaxis,:]\n",
    "                                    \n",
    "print(np.shape(train_x), np.shape(train_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - L-layer Neural Network\n",
    "\n",
    "**Question**: Use the helper functions you have implemented previously to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*. The functions you may need and their inputs are:\n",
    "```python\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    ...\n",
    "    return parameters\n",
    "def L_model_forward(X, parameters):\n",
    "    ...\n",
    "    return AL, caches\n",
    "def compute_cost(AL, Y):\n",
    "    ...\n",
    "    return cost\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    ...\n",
    "    return grads\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) write helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] =  np.random.randn(layer_dims[l], layer_dims[l-1])/np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: calculate forward propagation through hidden layers\n",
    "\n",
    "This involves passing the input through a linear integration and non-linear activiation layer.\n",
    "Cache the weights (W) and biases for later, and pass the activiations on to the next layer.\n",
    "\n",
    "\n",
    "Z, cache = linear_forward(A, W, b)\n",
    "A, cache =  linear_activation_forward(A_prev, W, b, activation):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter\n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z = W.dot(A) + b\n",
    "\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0,Z)\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value\n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    else:\n",
    "        print(\"\\033[91mError! Please make sure you have passed the value correctly in the \\\"activation\\\" parameter\")\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: generate the complete forward model, inluding the output neuron.\n",
    "L1: [LINEAR -> RELU] ->\n",
    "L2: [LINEAR -> RELU] ->\n",
    "L3: [LINEAR -> SIGMOID]  ->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Calculate the mismatch (cost) between the output predictions (AL) and the target values (nn_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: calculate the backwards propagation through the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    else:\n",
    "        print(\"\\033[91mError! Please make sure you have passed the value correctly in the \\\"activation\\\" parameter\")\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: add it to a backwards propagation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: use the new gradients to modify the weights and biases of the nework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "\n",
    "    accuracy = str(np.sum((p == y)/m))\n",
    "    # print results\n",
    "    # print (\"predictions: \" + str(p))\n",
    "    # print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "\n",
    "    return p, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 8:  Build Full Model & run beta test\n",
    "\n",
    "\n",
    "Step 8: parameters, cost = L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 1, print_cost=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "\n",
    "    # Parameters initialization.\n",
    "    parameters =initialize_parameters_deep(layers_dims)\n",
    "\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 500 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3) Train Model & Generate Predictions\n",
    "\n",
    "X input should be (\n",
    "b\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers_dim =  (5, 5, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "### Constants for 3-layer Neural Network\n",
    "input_dim = np.shape(train_x)\n",
    " \n",
    "m = input_dim[1]\n",
    "n_x = input_dim[0]  \n",
    "n_y = 1 \n",
    "\n",
    "layers_dim = (n_x, 5,3,1)\n",
    "learning_rate = 0.005\n",
    "\n",
    "print('layers_dim = ', layers_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.72642933, -0.27358579, -0.23620559, -0.47984616,  0.38702206],\n",
      "       [-1.0292794 ,  0.78030354, -0.34042208,  0.14267862, -0.11152182],\n",
      "       [ 0.65387455, -0.92132293, -0.14418936, -0.17175433,  0.50703711],\n",
      "       [-0.49188633, -0.07711224, -0.39259022,  0.01887856,  0.26064289],\n",
      "       [-0.49221186,  0.51193601,  0.40320363,  0.2247223 ,  0.40287503]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[-0.30577239, -0.05495818, -0.41848881, -0.11980319,  0.23718218],\n",
      "       [-0.30932009, -0.17743357, -0.30731297, -0.37798745, -0.3001904 ],\n",
      "       [-0.00566378, -0.49967638,  0.10483389,  0.7422861 ,  0.33185224]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[-0.11075631, -0.51247282, -0.43137204]]), 'b3': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "parameters =  initialize_parameters_deep(layers_dim)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6831277289877913\n",
      "Accuracy: 0.6362359550561798\n"
     ]
    }
   ],
   "source": [
    "# Test helper funcion: parameters = def initialize parameters(layers_dim)\n",
    "\n",
    "parameters, costs = L_layer_model(train_x, train_y, layers_dim, num_iterations = 1, learning_rate = learning_rate, print_cost = True)\n",
    "\n",
    "pred_train, accuracy = predict(train_x, train_y, parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Deep Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6831277289877913\n",
      "Accuracy: 0.5363128491620112\n",
      "(5, 179) (1, 179)\n"
     ]
    }
   ],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layers_dim, num_iterations = 1, learning_rate = learning_rate, print_cost = True)\n",
    "pred_test = predict(val_x, val_y, parameters)\n",
    "print(np.shape(val_x),np.shape(val_y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 1999: 0.5212182391940341\n",
      "Accuracy: 0.794943820224719\n",
      "Accuracy: 0.7150837988826816\n",
      "Cost after iteration 2099: 0.5171351642534391\n",
      "Accuracy: 0.797752808988764\n",
      "Accuracy: 0.7206703910614526\n",
      "Cost after iteration 2199: 0.5133336290798147\n",
      "Accuracy: 0.8047752808988764\n",
      "Accuracy: 0.7206703910614526\n",
      "Cost after iteration 2299: 0.5097884931403207\n",
      "Accuracy: 0.8103932584269662\n",
      "Accuracy: 0.7262569832402235\n",
      "Cost after iteration 2399: 0.5064735385224884\n",
      "Accuracy: 0.8132022471910112\n",
      "Accuracy: 0.7262569832402235\n",
      "Cost after iteration 2499: 0.5033623582906309\n",
      "Accuracy: 0.8117977528089888\n",
      "Accuracy: 0.7262569832402235\n",
      "Cost after iteration 2599: 0.5004375132813866\n",
      "Accuracy: 0.8146067415730337\n",
      "Accuracy: 0.7262569832402235\n",
      "Cost after iteration 2699: 0.49769093129472114\n",
      "Accuracy: 0.8188202247191011\n",
      "Accuracy: 0.7206703910614525\n",
      "Cost after iteration 2799: 0.4951084256975978\n",
      "Accuracy: 0.8202247191011236\n",
      "Accuracy: 0.7318435754189945\n",
      "Cost after iteration 2899: 0.4926745689350587\n",
      "Accuracy: 0.8202247191011236\n",
      "Accuracy: 0.7318435754189945\n",
      "Cost after iteration 2999: 0.4903776031890755\n",
      "Accuracy: 0.8230337078651685\n",
      "Accuracy: 0.7318435754189945\n",
      "Cost after iteration 3099: 0.4882026023251778\n",
      "Accuracy: 0.8230337078651686\n",
      "Accuracy: 0.7374301675977654\n",
      "Cost after iteration 3199: 0.4861473929399566\n",
      "Accuracy: 0.824438202247191\n",
      "Accuracy: 0.7430167597765363\n",
      "Cost after iteration 3299: 0.4842022547362159\n",
      "Accuracy: 0.827247191011236\n",
      "Accuracy: 0.7374301675977654\n",
      "Cost after iteration 3399: 0.4823592153903401\n",
      "Accuracy: 0.824438202247191\n",
      "Accuracy: 0.7430167597765363\n",
      "Cost after iteration 3499: 0.48061722788921846\n",
      "Accuracy: 0.824438202247191\n",
      "Accuracy: 0.7486033519553073\n",
      "Cost after iteration 3599: 0.47896679851959095\n",
      "Accuracy: 0.8230337078651685\n",
      "Accuracy: 0.7597765363128492\n",
      "Cost after iteration 3699: 0.4773963945620511\n",
      "Accuracy: 0.8202247191011236\n",
      "Accuracy: 0.7597765363128492\n",
      "Cost after iteration 3799: 0.4759007213190581\n",
      "Accuracy: 0.8202247191011236\n",
      "Accuracy: 0.7597765363128492\n",
      "Cost after iteration 3899: 0.47447698572566555\n",
      "Accuracy: 0.8160112359550562\n",
      "Accuracy: 0.7597765363128492\n",
      "Cost after iteration 3999: 0.47311934853748794\n",
      "Accuracy: 0.8160112359550562\n",
      "Accuracy: 0.7597765363128492\n",
      "Cost after iteration 4099: 0.47182143000126203\n",
      "Accuracy: 0.8174157303370786\n",
      "Accuracy: 0.7597765363128492\n",
      "Cost after iteration 4199: 0.4705840444126143\n",
      "Accuracy: 0.8160112359550562\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 4299: 0.46940593479920417\n",
      "Accuracy: 0.8146067415730337\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 4399: 0.4682776079503983\n",
      "Accuracy: 0.8146067415730337\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 4499: 0.46720101320566304\n",
      "Accuracy: 0.8132022471910112\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 4599: 0.4661673157013879\n",
      "Accuracy: 0.8117977528089888\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 4699: 0.4651756709267787\n",
      "Accuracy: 0.8117977528089888\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 4799: 0.46422268662539307\n",
      "Accuracy: 0.8117977528089888\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 4899: 0.4633135051754869\n",
      "Accuracy: 0.8089887640449438\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 4999: 0.462444072175786\n",
      "Accuracy: 0.8061797752808988\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 5099: 0.4616133014736198\n",
      "Accuracy: 0.8061797752808988\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 5199: 0.4608048221397119\n",
      "Accuracy: 0.8033707865168539\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 5299: 0.4600269689945815\n",
      "Accuracy: 0.8019662921348314\n",
      "Accuracy: 0.770949720670391\n",
      "Cost after iteration 5399: 0.45928036628762636\n",
      "Accuracy: 0.8019662921348314\n",
      "Accuracy: 0.770949720670391\n",
      "Cost after iteration 5499: 0.458561532366756\n",
      "Accuracy: 0.800561797752809\n",
      "Accuracy: 0.770949720670391\n",
      "Cost after iteration 5599: 0.45786516384800663\n",
      "Accuracy: 0.800561797752809\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 5699: 0.45718706440691853\n",
      "Accuracy: 0.800561797752809\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 5799: 0.45652585649663296\n",
      "Accuracy: 0.800561797752809\n",
      "Accuracy: 0.7653631284916201\n",
      "Cost after iteration 5899: 0.45588171964673746\n",
      "Accuracy: 0.8019662921348314\n",
      "Accuracy: 0.7653631284916201\n"
     ]
    }
   ],
   "source": [
    "for n_it in range(2000, 6000, 100):\n",
    "    \n",
    "    parameters, costs = L_layer_model(train_x, train_y, layers_dim, num_iterations = n_it, learning_rate = learning_rate, print_cost = False)    \n",
    "    pred_test, accuracy = predict(train_x, train_y, parameters)\n",
    "     \n",
    "    pred_test, accuracy = predict(val_x, val_y, parameters)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0) Winning Model: Selection, Testing, Submission\n",
    "\n",
    "Generate predictions for competition test dataset with unknown ground-truth labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_target_predict = [xxxx].predict(test_df_features_norm[columns_final_features])\n",
    " \n",
    "#print('Predicted result: ', y_target_predict)\n",
    "#print(len(y_target_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission = pd.DataFrame({'PassengerId':test_df_features_Match.PassengerId.values,'Survived':y_target_predict})\n",
    "#submission.Survived = submission.Survived.astype(int)\n",
    "\n",
    "#print(submission.head()) # make sure we are submitting integers and not floats\n",
    "#print(submission.shape)\n",
    "\n",
    "#filename = 'Titanic Predictions_pub.csv'\n",
    "#submission.to_csv(filename, index=False)\n",
    "#print('Saved file: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
