{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jhynes/titanic-standard-classifier-test?scriptVersionId=144422216\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n \nimport seaborn as sns\nimport os\n\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nimport sklearn\nimport mpl_toolkits\n\nprint(f\"Found TF-DF {tfdf.__version__}\")\n\n!pwd; \n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:30.890185Z","iopub.execute_input":"2023-09-27T08:58:30.890624Z","iopub.status.idle":"2023-09-27T08:58:32.029941Z","shell.execute_reply.started":"2023-09-27T08:58:30.89059Z","shell.execute_reply":"2023-09-27T08:58:32.028461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.0) Load Data & Inspect:\n* Load data from CSV files, typically test/train\n* Make copy of data\n* Examine Variables using info(), describe(), head(), isnull().sum(), list()\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n\n#train_df = train_df.copy()\n\n#test_df = test_df.copy()\n\n\nexample_submission_df = pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\nprint(test_df.isnull().sum()) # inspect data types: any missing or null data \nprint(train_df.isnull().sum()) # inspect data types: any missing or null data \n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:32.032454Z","iopub.execute_input":"2023-09-27T08:58:32.033399Z","iopub.status.idle":"2023-09-27T08:58:32.064876Z","shell.execute_reply.started":"2023-09-27T08:58:32.033362Z","shell.execute_reply":"2023-09-27T08:58:32.064059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.0) Examine Data & Pre-Processing: Understanding the Data ","metadata":{}},{"cell_type":"markdown","source":"### 2.1) Data cleaning\n\n* replace missing values with expected values  \n    * Age: the distribution of ages is non-gaussian, indicating that we should probably mean-fill the ages in a stratified way, but for now we will just assume a normal distribition. \n    *  Channge Categorical to numerical\n    * replace nans with mean, mode, 0, etc. \n \nFor now, let's just examine the feature columns that are easy to work with. Here are our columns/features: \n* survival: Survival\n* pclass: Ticket class\n* sex: Sex\n* Age: Age in years\n* sibsp: # of siblings / spouses aboard the Titanic\n* parch: # of parents / children aboard the Titanic\n* ticket: Ticket number\n* fare: Passenger fare\n* embarked: Port of embarkation\n\nWe are ignoring \"Cabin numbe\"r for now as there are many missing values and cabin number\nlikely reflects redundant information about 1st, 2nd, 3rd class numbers. However, individuals in\n2nd/3rd class cabins that were not near the hull breach are more likely to have survived. So it could be\nuseful information. \n","metadata":{}},{"cell_type":"code","source":"train_df['Embarked'].fillna(0, inplace=True)\ntrain_df['Embarked'].replace('Q', 1,inplace=True)\ntrain_df['Embarked'].replace('S', 2,inplace=True)\ntrain_df['Embarked'].replace('C', 3,inplace=True)\n\ntest_df['Embarked'].fillna(0, inplace=True)\ntest_df['Embarked'].replace('Q', 1,inplace=True)\ntest_df['Embarked'].replace('S', 2,inplace=True)\ntest_df['Embarked'].replace('C', 3,inplace=True)\n\ntrain_df['Sex'].replace('male', 0,inplace=True)\ntrain_df['Sex'].replace('female', 1,inplace=True)\n\ntest_df['Sex'].replace('male', 0,inplace=True)\ntest_df['Sex'].replace('female', 1,inplace=True)\n\n\ntest_df['Fare'].fillna(test_df['Fare'].mean(),inplace=True)\n#train_df['Fare'].fillna(test_df['Fare'].mode(),inplace=True)\n\nprint(test_df.isnull().sum()) # inspect data types: any missing or null data \nprint(train_df.isnull().sum()) # inspect data types: any missing or null data  \n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:32.066454Z","iopub.execute_input":"2023-09-27T08:58:32.067101Z","iopub.status.idle":"2023-09-27T08:58:32.091649Z","shell.execute_reply.started":"2023-09-27T08:58:32.067068Z","shell.execute_reply":"2023-09-27T08:58:32.090432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_age_unfilled = train_df.copy(deep = True) \n\nmean = train_df['Age'].mean() \nstd = train_df['Age'].std()\nmode_nsurvived= train_df['Age'].mode()\nmode_nsurvived_test= test_df['Age'].mode()\n\n \nnum_null = train_df['Age'].isnull().sum()\nlist_null = train_df['Age'].isnull()\n\n# sample from distribution of ages but ignore nan\n#resample_index_train = train_df_age_unfilled['Age'].isnull()==0 \n#resample_index_test = test_df_age_unfilled['Age'].isnull()==0 \n\n#print(train_df_age_unfilled['Age'])\n\n#filled_resample_train = train_df_age_unfilled['Age'].sample( n=891, replace=False, weights = resample_index_train.astype(int))\n#filled_resample_test = test_df_age_unfilled['Age'].sample( n=len(resample_index_test), replace=False, weights = resample_index_test.astype(int)) \n\n\ntrain_df[\"Age\"].fillna(value=mean, inplace=True) # remove negative age values\ntest_df[\"Age\"].fillna(value=mean, inplace=True) # remove negative age values\n\n # print(train_df_features_survivedId[\"Age\"])\nprint('AGE DATA CLEANING ...') # inspect data types: any missing or null data \nprint(test_df.isnull().sum()) # inspect data types: any missing or null data \nprint(train_df.isnull().sum()) # inspect data types: any missing or null data ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:32.09456Z","iopub.execute_input":"2023-09-27T08:58:32.094955Z","iopub.status.idle":"2023-09-27T08:58:32.122553Z","shell.execute_reply.started":"2023-09-27T08:58:32.094921Z","shell.execute_reply":"2023-09-27T08:58:32.121123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inspect Consequences of Data Cleaning:\n\n1) How does the predictive value of age change after filling the data?\nDoes it decrease, increase, or stay the same. Ideally it should match the predictive value \nof the original the original data. Try different methods to ensure this. \n*     filling with zeros reduces predictive power = 0.12\n*     filling with mean increases preditive power = 1.43\n*     filling with mode = 0.93\n*     backfilling = 1.12\n\n2) Does the distribution of the data change?\nThe age distribution is non-gaussian. If we fill the nans using mean sampled data, \nare we underestimating the impact of age on survival for a particular demographic (lower tail). \nAlternatively, we add more outliers, this can cause data crowding during normalization or dim reduction. ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n\n# Checking best features\ncolumns_to_be_added_as_features = ['Sex', 'Pclass', 'Fare' ,'Embarked' ,'Parch' ,'SibSp', 'Age']\n\nselector = SelectKBest(f_classif, k='all')\n\nselector.fit(train_df[columns_to_be_added_as_features],train_df['Survived'])\nscores = -np.log10(selector.pvalues_)\nindices = np.argsort(scores)[::-1]\n\nprint('Features importance:')\nfor i in range(len(scores)):\n    print('%.2f %s' % (scores[indices[i]], columns_to_be_added_as_features[indices[i]]))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:32.124121Z","iopub.execute_input":"2023-09-27T08:58:32.124967Z","iopub.status.idle":"2023-09-27T08:58:32.14045Z","shell.execute_reply.started":"2023-09-27T08:58:32.124932Z","shell.execute_reply":"2023-09-27T08:58:32.139318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note: the distribution of ages is non-gaussian, \n# indicating that we should probably mean-fill the ages in a stratified way, \n# but for now we will just assume a normal distribition. \nfig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n\n# stats of the data\ntrain_df.describe()     # inspect data types \n# Plot distributions of ages to before and after filling \nsns.histplot(train_df_age_unfilled , x = 'Age', ax=ax1 ).set(title='Train AGE dist  - before mean fill nans')\nsns.histplot(train_df, x = 'Age',ax=ax2, color='m').set(title='Train AGE dist - after bfill nans ')\nsns.histplot(test_df , x = 'Age', ax=ax3 ).set(title='Test Age - after mean fill nans ')\n#sns.histplot(test_df, x = 'Age',ax=ax4, color='m').set(title='Test Age - after bfill nans')\n\n\nsns.set(rc={\"figure.figsize\":(15, 6)})  ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:32.142013Z","iopub.execute_input":"2023-09-27T08:58:32.143132Z","iopub.status.idle":"2023-09-27T08:58:33.5864Z","shell.execute_reply.started":"2023-09-27T08:58:32.143086Z","shell.execute_reply":"2023-09-27T08:58:33.585385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.0) Exploratory Data Analysis: select most informative features\n \n* Univariate Analysis (feature decoding)\n* Bivariate Analysis (relational plot) \n* Latent representation learning/inspection (PCA, tSNE, etc)\n* Feature Engineering (normalization)\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n\n# Checking best features\ncolumns_to_be_added_as_features = ['Sex', 'Pclass', 'Fare' ,'Embarked' ,'Parch' ,'SibSp', 'Age']\n\nselector = SelectKBest(f_classif, k='all')\n\nselector.fit(train_df[columns_to_be_added_as_features], train_df['Survived'])\nscores = -np.log10(selector.pvalues_)\nindices = np.argsort(scores)[::-1]\n\nprint('Features importance:')\nfor i in range(len(scores)):\n    print('%.2f %s' % (scores[indices[i]], columns_to_be_added_as_features[indices[i]]))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:33.588373Z","iopub.execute_input":"2023-09-27T08:58:33.588839Z","iopub.status.idle":"2023-09-27T08:58:33.604667Z","shell.execute_reply.started":"2023-09-27T08:58:33.588796Z","shell.execute_reply":"2023-09-27T08:58:33.603393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_features = train_df[columns_to_be_added_as_features + ['Survived']]\ntest_df_features = test_df[columns_to_be_added_as_features] \n\n# For submission scoring (i.e., don't normalize 'PassenderID' feature during subsequent feature engineering steps)\ntest_df_features_Match = test_df[columns_to_be_added_as_features +  ['PassengerId']] ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:33.606139Z","iopub.execute_input":"2023-09-27T08:58:33.606534Z","iopub.status.idle":"2023-09-27T08:58:33.617545Z","shell.execute_reply.started":"2023-09-27T08:58:33.606503Z","shell.execute_reply":"2023-09-27T08:58:33.615936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normalize Features in Train & Test Data. \nDon't normalize passenger information!\nThis normalization (0-1) will not affect surviver id info as it is already 0s and 1s","metadata":{}},{"cell_type":"code","source":"def normalize(df):\n    result = df.copy()\n    for feature_name in df.columns:\n        max_value = df[feature_name].max()\n        min_value = df[feature_name].min()\n        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n    return result\n\n# New df\ntrain_df_features_norm = normalize(train_df_features)\ntest_df_features_norm = normalize(test_df_features)\n\n\nprint(train_df_features_norm.columns.values)\nprint(test_df_features_norm.columns.values)\ntrain_df_features_norm.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:33.61956Z","iopub.execute_input":"2023-09-27T08:58:33.619944Z","iopub.status.idle":"2023-09-27T08:58:33.664579Z","shell.execute_reply.started":"2023-09-27T08:58:33.619911Z","shell.execute_reply":"2023-09-27T08:58:33.66332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data visualization, latent factors, feature engineering\n\nVisualizing the structure of the data and the relationship of the variables is\nuseful for deciding which ML model to use. \n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Relational plot\ng = sns.pairplot(train_df_features_norm, hue = 'Survived', corner=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:33.669086Z","iopub.execute_input":"2023-09-27T08:58:33.669897Z","iopub.status.idle":"2023-09-27T08:58:53.10965Z","shell.execute_reply.started":"2023-09-27T08:58:33.669864Z","shell.execute_reply":"2023-09-27T08:58:53.108452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nnum_comp = 4\n\n# python PCA\npca_out = PCA(n_components=num_comp)\npca_out.fit(train_df_features_norm[columns_to_be_added_as_features])\npca_out_data = pca_out.fit_transform(train_df_features_norm[columns_to_be_added_as_features])\n\nprint(f'explained variance = {pca_out.explained_variance_ratio_} ')\nprint(f'cumulative variance  = {np.cumsum(pca_out.explained_variance_ratio_)}' )\n ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:53.110977Z","iopub.execute_input":"2023-09-27T08:58:53.111357Z","iopub.status.idle":"2023-09-27T08:58:53.130748Z","shell.execute_reply.started":"2023-09-27T08:58:53.111326Z","shell.execute_reply":"2023-09-27T08:58:53.129588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nloadings = pca_out.components_ \nvar = pca_out.explained_variance_ratio_\npca_cumSum = np.cumsum(var)\n\npc_list = [\"PC\"+str(i) for i in list(range(1, num_comp+1))]\n\n# component loadings or weights (correlation coefficient between original variables and the component) \n# component loadings represents the elements of the eigenvector\n# the squared loadings within the PCs always sums to 1\n\n\nloadings_pca_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))\nloadings_pca_df['variable'] = train_df_features_norm[columns_to_be_added_as_features].columns.values\nloadings_pca_df = loadings_pca_df.set_index('variable')\n\npca_cumSum_df = pd.DataFrame({'var':var,'cumsum_var':pca_cumSum, 'PC':pc_list})\n\n\npca_plot_df = pd.DataFrame({'PC1': pca_out_data[:,0],\n                   'PC2': pca_out_data[:,1],\n                   'PC3': pca_out_data[:,2],\n                   'PC4': pca_out_data[:,3],  \n                   'Survived': train_df_features_norm['Survived']\n                  })","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:53.132274Z","iopub.execute_input":"2023-09-27T08:58:53.132678Z","iopub.status.idle":"2023-09-27T08:58:53.146553Z","shell.execute_reply.started":"2023-09-27T08:58:53.132641Z","shell.execute_reply":"2023-09-27T08:58:53.144882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figA, (ax1, ax2) = plt.subplots(1,2, figsize=(10, 3))\n\n# Variance explained\nsns.barplot(x='PC',y=\"var\", data=pca_cumSum_df, color=\"c\", ax=ax1).set(title='PCA Explained Variance');\nsns.barplot(x='PC',y=\"cumsum_var\", data=pca_cumSum_df, color=\"c\", ax=ax2).set(title='PCA Explained Variance');\n\nsns.pairplot(pca_plot_df, hue=\"Survived\" )\n\n#figB2, (axB1, axB2, axB3) = plt.subplots(1,3, figsize=(10, 4))\n\n#sns.scatterplot(data=pca_plot_df, x='PC1',y=\"PC2\",  hue=\"Survived\" ,ax=axB1).set(title='PCA Plot');\n#sns.scatterplot(data=pca_plot_df, x='PC1',y=\"PC3\",  hue=\"Survived\", ax=axB2).set(title='PCA Plot');\n#sns.scatterplot(data=pca_plot_df, x='PC1',y=\"PC6\",  hue=\"Survived\", ax=axB3).set(title='PCA Plot');","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:58:53.147859Z","iopub.execute_input":"2023-09-27T08:58:53.148808Z","iopub.status.idle":"2023-09-27T08:59:04.107506Z","shell.execute_reply.started":"2023-09-27T08:58:53.148774Z","shell.execute_reply":"2023-09-27T08:59:04.106297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## 4.0) Model Training\n\nAfter examining the structure of the input data space, we see that it has some\nstructure (e.g., clustered target data) but that it is complex,\ne.g., non-gaussian, nested hierarchies, outliers, non-linearities. \n\nAn SVM, KNN, or decision tree may may get us 70% of the way there but they may run into problems with\nover generalizing or overfitting. Some perturbation, sub-ensemble, or penalties may need to be introducted.  \nPerhaps some more complex feature engineering could be introducted. \n\n* Support Vector Machine \n* Decision Tree\n* K-Nearest Neighbours\n\n(not included yet...)\n- Bagging Decision Tree (Ensemble Learning I)\n- Boosted Decision Tree (Ensemble Learning II)\n- Random Forest (Ensemble Learning III)\n- Naive Bayes\n- Logistic Regression\n- Voting Classification (Ensemble Learning IV)\n- Neural Network (Deep Learning)\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### Splitting Training Data into Test/Validation Sets\nBelow, the validation's ratio was set to 0.2, which means 20% of data will be used to validate it. \nTo split the dataset into training and validation sets, we will be using Sklearn's train_test_split method. \nThen we will be splitting the features and labels as shown in the last 4 lines of the code piece below.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n ## SWAP OUT REG DATA FOR PCA ###\n#train_df_norm = pca_plot_df[['PC1', 'PC2','PC3','PC4', 'Survived']]\n\n#columns_to_be_added_as_features = ['PC1', 'PC2',  'PC3', 'PC4']\n #### SWAP OUT REG DATA FOR PCA  ###\n\n    \ntrain_df_features_norm = train_df_features_norm.sample(frac=1).reset_index(drop=True)\n\n\nvalidation_set_ratio = 0.2  # 20\nvalidation_set_size = int(len(train_df_features_norm)*validation_set_ratio)\ntraining_set_size = len(train_df_features_norm) - validation_set_size\n\nprint(\"Total set size: {}\".format(len(train_df_features_norm)))\nprint(\"Training set size: {}\".format(training_set_size))\nprint(\"Validation set size: {}\".format(validation_set_size))\n\n\n# test vs train = 20% split\ntrain, val = train_test_split(train_df_features_norm, test_size=validation_set_ratio)\n\ntrain_X = train[columns_to_be_added_as_features]\ntrain_Label = train['Survived']\n\nval_X = val[columns_to_be_added_as_features]\nval_Label = val['Survived']","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:59:04.109233Z","iopub.execute_input":"2023-09-27T08:59:04.109563Z","iopub.status.idle":"2023-09-27T08:59:04.125809Z","shell.execute_reply.started":"2023-09-27T08:59:04.109533Z","shell.execute_reply":"2023-09-27T08:59:04.12464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVM Model: training, predictions, and visualizing","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n\nSVM_KERNEL = \"linear\"\nSVM_C = 10\nSVM_GAMMA = 0.00001\n\nsvm_model = SVC(kernel = SVM_KERNEL, C = SVM_C, gamma = SVM_GAMMA)\nsvm_model.fit(train_X, train_Label.values.ravel())\n\n\ny_pred_svm = svm_model.predict(val_X)\naccuracy = metrics.accuracy_score(val_Label, y_pred_svm)\nprecision = metrics.precision_score(val_Label, y_pred_svm)\nrecall = metrics.recall_score(val_Label, y_pred_svm)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\ncv_score = cross_val_score(svm_model, train_df_features_norm[columns_to_be_added_as_features], train_df_features_norm['Survived'], cv = 6  )\nprint(\"Cross Val Alt\", cv_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:59:04.127259Z","iopub.execute_input":"2023-09-27T08:59:04.127679Z","iopub.status.idle":"2023-09-27T08:59:04.286687Z","shell.execute_reply.started":"2023-09-27T08:59:04.127648Z","shell.execute_reply":"2023-09-27T08:59:04.285334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Model: training, predictions, and visualizing","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\n\n# Decision tree\ndecision_tree_model = tree.DecisionTreeClassifier()\ndecision_tree_model = decision_tree_model.fit(train_X, train_Label.values.ravel())\ny_pred_tree = decision_tree_model.predict(val_X)\n\n#tree.plot_tree(y_pred_svm)\n\naccuracy = metrics.accuracy_score(val_Label, y_pred_tree)\nprecision = metrics.precision_score(val_Label, y_pred_tree)\nrecall = metrics.recall_score(val_Label, y_pred_tree)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\n\n# Alt Cross Val Method\ncv_score = cross_val_score(decision_tree_model, train_df_features_norm[columns_to_be_added_as_features], train_df_features_norm['Survived'], cv = 6  )\nprint(\"Cross Val Alt\", cv_score)\n\n\n#print(\"Accuracy:\",metrics.accuracy_score(val_Label, y_pred_tree))","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:59:04.288352Z","iopub.execute_input":"2023-09-27T08:59:04.289259Z","iopub.status.idle":"2023-09-27T08:59:04.362277Z","shell.execute_reply.started":"2023-09-27T08:59:04.289221Z","shell.execute_reply":"2023-09-27T08:59:04.361415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### KNN Model: training, predictions, and visualizing\n \n A for loop can be created to check for the optimal value of k \"local neighbourhood\".  \n I found that 3 and 9 worked equally well in a tested range of k= [1:2:15]","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\naccuracy = []\n \nknn = KNeighborsClassifier(n_neighbors=3)\n\nknn.fit(train_X, train_Label.values.ravel())\n\ny_pred_knn = knn.predict(val_X)\n\n\naccuracy.append( metrics.accuracy_score(val_Label, y_pred_knn))\nprecision = metrics.precision_score(val_Label, y_pred_knn)\nrecall = metrics.recall_score(val_Label, y_pred_knn)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\n# Alt Cross Val Method\ncv_score = cross_val_score(knn, train_df_features_norm[columns_to_be_added_as_features], train_df_features_norm['Survived'], cv = 5  )\nprint(\"Cross Val Alt\", cv_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:59:04.363716Z","iopub.execute_input":"2023-09-27T08:59:04.364281Z","iopub.status.idle":"2023-09-27T08:59:04.509212Z","shell.execute_reply.started":"2023-09-27T08:59:04.364249Z","shell.execute_reply":"2023-09-27T08:59:04.507893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.0) Winning Model: Selection, Testing, Submission\n\nGenerate predictions for competition test dataset with unknown ground-truth labels. \n\nAs a first pass, we will submit for the SVM model. \n","metadata":{}},{"cell_type":"code","source":"y_target_predict = knn.predict(test_df_features_norm[columns_to_be_added_as_features]) \n#y_target_predict_svm = svm_model.predict(test_df_features_norm[columns_to_be_added_as_features])  \n\nprint('Predicted result: ', y_target_predict)\nprint(len(y_target_predict))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:59:04.511041Z","iopub.execute_input":"2023-09-27T08:59:04.511385Z","iopub.status.idle":"2023-09-27T08:59:04.558407Z","shell.execute_reply.started":"2023-09-27T08:59:04.511354Z","shell.execute_reply":"2023-09-27T08:59:04.55717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId':test_df_features_Match.PassengerId.values,'Survived':y_target_predict})\nsubmission.Survived = submission.Survived.astype(int)\n\nprint(submission.head()) # make sure we are submitting integers and not floats\nprint(submission.shape)\n\nfilename = 'Titanic Predictions_pub.csv'\nsubmission.to_csv(filename, index=False)\nprint('Saved file: ' + filename)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T09:00:09.49615Z","iopub.execute_input":"2023-09-27T09:00:09.497359Z","iopub.status.idle":"2023-09-27T09:00:09.51376Z","shell.execute_reply.started":"2023-09-27T09:00:09.497315Z","shell.execute_reply":"2023-09-27T09:00:09.512239Z"},"trusted":true},"execution_count":null,"outputs":[]}]}