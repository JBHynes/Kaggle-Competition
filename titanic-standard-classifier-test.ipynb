{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jhynes/titanic-standard-classifier-test?scriptVersionId=144561339\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n \nimport seaborn as sns\nimport os\n\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nimport sklearn\nimport mpl_toolkits\n\nprint(f\"Found TF-DF {tfdf.__version__}\")\n\n!pwd; ","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:10.9733Z","iopub.execute_input":"2023-09-28T12:36:10.974051Z","iopub.status.idle":"2023-09-28T12:36:12.106894Z","shell.execute_reply.started":"2023-09-28T12:36:10.973982Z","shell.execute_reply":"2023-09-28T12:36:12.105301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Overview:\n\n### V1.0 Baseline Submisson\nBaseline test useing basic classifiers (SVM, decision trees, KNN) and minimal feature engineering. \n\n### Next Updates\nV2.0 will focus on vector-based feature engineering and node embedding encoding-decoding models. \n\n\n","metadata":{}},{"cell_type":"markdown","source":"## 1.0) Load Data & Inspect:\n* Load data from CSV files, typically test/train\n* Make copy of data\n* Examine Variables using info(), describe(), head(), isnull().sum(), list()\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n\n#train_df = train_df.copy()\n\n#test_df = test_df.copy()\n\n\nexample_submission_df = pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:12.109928Z","iopub.execute_input":"2023-09-28T12:36:12.1107Z","iopub.status.idle":"2023-09-28T12:36:12.152442Z","shell.execute_reply.started":"2023-09-28T12:36:12.110637Z","shell.execute_reply":"2023-09-28T12:36:12.151438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1) Data cleaning\n \n\n\n* Convert categorical data to numerical (float)\n* Missing Data: \n    * Replace missing values or NaNs with values that are both statistically and pragmatically appropriate.\n    * Replace nans with mean, mode, 0, etc. \n    * Age: the distribution of ages is non-gaussian, indicating that we should probably mean-fill the ages in a stratified way, but for now we will just assume a normal distribition and use a simple fill method. \n    \n* Ignore for now:\n    * Cabin Number: We are ignoring \"Cabin number\" for now as there are many missing values. Cabin letter/number coding likely reflects redundant information related to 1st, 2nd, 3rd class tickest (lower class ticket holders were likely in cabins in the lower decks). That said, individuals in 2nd/3rd class cabins that were furthest away from the hull breach would have increased chances of survival, so come in use in the future.\n\n* Feature Selection:\nFor now, let's just examine the feature columns that are easy to work with. Here are our columns/features: \n* survival: Survival\n* pclass: Ticket class\n* sex: Sex\n* Age: Age in years\n* sibsp: # of siblings / spouses aboard the Titanic\n* parch: # of parents / children aboard the Titanic\n* ticket: Ticket number\n* fare: Passenger fare\n* embarked: Port of embarkation\n","metadata":{}},{"cell_type":"code","source":"print(test_df.isnull().sum()) # inspect data types: any missing or null data \nprint(train_df.isnull().sum()) # inspect data types: any missing or null data \n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:12.153614Z","iopub.execute_input":"2023-09-28T12:36:12.153907Z","iopub.status.idle":"2023-09-28T12:36:12.164221Z","shell.execute_reply.started":"2023-09-28T12:36:12.153882Z","shell.execute_reply":"2023-09-28T12:36:12.162867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Interpretation:\nThe plots below tell us that class, sex, age, and sibling number are predictive of an individuals survival.  \n\nAlthough are many confounding relationships among the variables that warrant caution when making an interpretation, some noteable trends and possible interpretations include:\n- women in first class were most likely to survive.\n- women in 3rd class had notable poorer survival rate than women in 1st/2nd class. \n- men in 3rd class were had poorer survival over all. \n- very young males/female in 1st/2nd had better chance of survival. This was not the case in 3rd class. \n\n","metadata":{}},{"cell_type":"code","source":"fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2,2, figsize=(12,5))\n\nsns.countplot(data=train_df, x=\"Sex\", hue=\"Survived\", ax=ax1)\nsns.countplot(data=train_df, x=\"Pclass\", hue=\"Survived\", ax=ax2)\nsns.countplot(data=train_df, x=\"Embarked\", hue=\"Survived\", ax=ax3)\nsns.countplot(data=train_df, x=\"Parch\", hue=\"Survived\", ax=ax4)\nfig.tight_layout()\n\nfig, plt.figure(figsize=(12,5))\nsns.catplot(data=train_df, y=\"Age\", x=\"Pclass\", col=\"Sex\", hue = \"Survived\", ax=ax4)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:12.167684Z","iopub.execute_input":"2023-09-28T12:36:12.168113Z","iopub.status.idle":"2023-09-28T12:36:14.783365Z","shell.execute_reply.started":"2023-09-28T12:36:12.168072Z","shell.execute_reply":"2023-09-28T12:36:14.782212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Embarked'].fillna('S', inplace=True)  # unkown \ntrain_df['Embarked'].replace('Q', 1,inplace=True)\ntrain_df['Embarked'].replace('S', 2,inplace=True)\ntrain_df['Embarked'].replace('C', 3,inplace=True)\n\ntest_df['Embarked'].fillna('S', inplace=True)\ntest_df['Embarked'].replace('Q', 1,inplace=True)\ntest_df['Embarked'].replace('S', 2,inplace=True)\ntest_df['Embarked'].replace('C', 3,inplace=True)\n\ntrain_df['Sex'].replace('male', 0,inplace=True)\ntrain_df['Sex'].replace('female', 1,inplace=True)\n\ntest_df['Sex'].replace('male', 0,inplace=True)\ntest_df['Sex'].replace('female', 1,inplace=True)\n\n\ntest_df['Fare'].fillna(test_df['Fare'].mean(),inplace=True)\n#train_df['Fare'].fillna(test_df['Fare'].mode(),inplace=True)\n\n\n\n# Dropping the rows including NaN values.\ntrain_df.dropna(subset=[\"Age\"], inplace=True)\ntest_df[\"Age\"].fillna(method ='ffill', inplace=True) # remove negative age values\n\n\nprint(test_df.isnull().sum()) # inspect data types: any missing or null data \nprint(train_df.isnull().sum()) # inspect data types: any missing or null data  ","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:14.784793Z","iopub.execute_input":"2023-09-28T12:36:14.785105Z","iopub.status.idle":"2023-09-28T12:36:14.812515Z","shell.execute_reply.started":"2023-09-28T12:36:14.785078Z","shell.execute_reply":"2023-09-28T12:36:14.811527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# UNCOMMMENT TO FILL DATA\n\ntrain_df_age_unfilled = train_df.copy(deep = True) \n\nmean = train_df['Age'].mean() \nstd = train_df['Age'].std()\nmode_nsurvived= train_df['Age'].mode()\nmode_nsurvived_test= test_df['Age'].mode()\n\n \nnum_null = train_df['Age'].isnull().sum()\nlist_null = train_df['Age'].isnull()\n\n# sample from distribution of ages but ignore nan\n#resample_index_train = train_df_age_unfilled['Age'].isnull()==0 \n#resample_index_test = test_df_age_unfilled['Age'].isnull()==0 \n\n#print(train_df_age_unfilled['Age'])\n\n#filled_resample_train = train_df_age_unfilled['Age'].sample( n=891, replace=False, weights = resample_index_train.astype(int))\n#filled_resample_test = test_df_age_unfilled['Age'].sample( n=len(resample_index_test), replace=False, weights = resample_index_test.astype(int)) \n\n\n#train_df[\"Age\"].fillna(value=mean, inplace=True) # remove negative age values\n#test_df[\"Age\"].fillna(value=mean, inplace=True) # remove negative age values\n\n # print(train_df_features_survivedId[\"Age\"])\nprint('AGE DATA CLEANING ...') # inspect data types: any missing or null data \nprint(test_df.isnull().sum()) # inspect data types: any missing or null data \nprint(train_df.isnull().sum()) # inspect data types: any missing or null data ","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:14.814153Z","iopub.execute_input":"2023-09-28T12:36:14.814463Z","iopub.status.idle":"2023-09-28T12:36:14.829054Z","shell.execute_reply.started":"2023-09-28T12:36:14.814434Z","shell.execute_reply":"2023-09-28T12:36:14.828136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inspect Consequences of Data Cleaning:\n\n1) How does the predictive value of age change after filling the data?\nDoes it decrease, increase, or stay the same. Ideally it should match the predictive value \nof the original the original data. Try different methods to ensure this. \n*     filling with zeros reduces predictive power = 0.12\n*     filling with mean increases preditive power = 1.43\n*     filling with mode = 0.93\n*     backfilling = 1.12\n\n2) Does the distribution of the data change?\nThe age distribution is non-gaussian. If we fill the nans using mean sampled data, \nare we underestimating the impact of age on survival for a particular demographic (lower tail). \nAlternatively, we add more outliers, this can cause data crowding during normalization or dim reduction. ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n\n# Checking best features\ncolumns_to_be_explored = ['Sex', 'Pclass' ,'Embarked' ,'Parch' , 'Age', 'SibSp']\n\nselector = SelectKBest(f_classif, k='all')\n\nselector.fit(train_df[columns_to_be_explored],train_df['Survived'])\nscores = -np.log10(selector.pvalues_)\nindices = np.argsort(scores)[::-1]\n\nprint('Features importance:')\nfor i in range(len(scores)):\n    print('%.2f %s' % (scores[indices[i]], columns_to_be_explored[indices[i]]))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:14.830278Z","iopub.execute_input":"2023-09-28T12:36:14.830691Z","iopub.status.idle":"2023-09-28T12:36:14.846648Z","shell.execute_reply.started":"2023-09-28T12:36:14.830643Z","shell.execute_reply":"2023-09-28T12:36:14.845597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note: the distribution of ages is non-gaussian, \n# indicating that we should probably mean-fill the ages in a stratified way, \n# but for now we will just assume a normal distribition. \nfig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n\n# stats of the data\ntrain_df.describe()     # inspect data types \n# Plot distributions of ages to before and after filling \nsns.histplot(train_df_age_unfilled , x = 'Age', ax=ax1 ).set(title='Train AGE dist  - before mean fill nans')\nsns.histplot(train_df, x = 'Age',ax=ax2, color='m').set(title='Train AGE dist - after bfill nans ')\nsns.histplot(test_df , x = 'Age', ax=ax3 ).set(title='Test Age - after mean fill nans ')\n#sns.histplot(test_df, x = 'Age',ax=ax4, color='m').set(title='Test Age - after bfill nans')\n\n\nsns.set(rc={\"figure.figsize\":(15, 6)})  ","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:14.848412Z","iopub.execute_input":"2023-09-28T12:36:14.84872Z","iopub.status.idle":"2023-09-28T12:36:16.1475Z","shell.execute_reply.started":"2023-09-28T12:36:14.848693Z","shell.execute_reply":"2023-09-28T12:36:16.146479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.0) Exploratory Data Analysis: select most informative features\n \n* Univariate Analysis (feature decoding)\n* Bivariate Analysis (relational plot) \n* Latent representation learning/inspection (PCA, tSNE, etc)\n* Feature Engineering (normalization)\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n\n# Checking best features\n \n\nselector = SelectKBest(f_classif, k='all')\n\nselector.fit(train_df[columns_to_be_explored], train_df['Survived'])\nscores = -np.log10(selector.pvalues_)\nindices = np.argsort(scores)[::-1]\n\nprint('Features importance:')\nfor i in range(len(scores)):\n    print('%.2f %s' % (scores[indices[i]], columns_to_be_explored[indices[i]]))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:16.149356Z","iopub.execute_input":"2023-09-28T12:36:16.149686Z","iopub.status.idle":"2023-09-28T12:36:16.16107Z","shell.execute_reply.started":"2023-09-28T12:36:16.149657Z","shell.execute_reply":"2023-09-28T12:36:16.159894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_be_added_as_features = ['Sex', 'Pclass' ,'Embarked' ,'Parch' ]\n\ntrain_df_features = train_df[columns_to_be_added_as_features + ['Survived']]\ntest_df_features = test_df[columns_to_be_added_as_features] \n\n# For submission scoring (i.e., don't normalize 'PassenderID' feature during subsequent feature engineering steps)\ntest_df_features_Match = test_df[columns_to_be_added_as_features +  ['PassengerId']] ","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:16.166983Z","iopub.execute_input":"2023-09-28T12:36:16.167441Z","iopub.status.idle":"2023-09-28T12:36:16.176743Z","shell.execute_reply.started":"2023-09-28T12:36:16.167409Z","shell.execute_reply":"2023-09-28T12:36:16.175676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normalize Features in Train & Test Data. \nDon't normalize passenger information!\nThis normalization (0-1) will not affect surviver id info as it is already 0s and 1s","metadata":{}},{"cell_type":"code","source":"def normalize(df):\n    result = df.copy()\n    for feature_name in df.columns:\n        max_value = df[feature_name].max()\n        min_value = df[feature_name].min()\n        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n    return result\n\n# New df\ntrain_df_features_norm = normalize(train_df_features)\ntest_df_features_norm = normalize(test_df_features)\n\n\nprint(train_df_features_norm.columns.values)\nprint(test_df_features_norm.columns.values)\ntrain_df_features_norm.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:16.177858Z","iopub.execute_input":"2023-09-28T12:36:16.178132Z","iopub.status.idle":"2023-09-28T12:36:16.208261Z","shell.execute_reply.started":"2023-09-28T12:36:16.178107Z","shell.execute_reply":"2023-09-28T12:36:16.207327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data visualization, latent factors, feature engineering\n\nVisualizing the structure of the data and the relationship of the variables is\nuseful for deciding which ML model to use. \n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Relational plot\ng = sns.pairplot(train_df_features_norm, hue = 'Survived', corner=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:16.209572Z","iopub.execute_input":"2023-09-28T12:36:16.209865Z","iopub.status.idle":"2023-09-28T12:36:23.028926Z","shell.execute_reply.started":"2023-09-28T12:36:16.209839Z","shell.execute_reply":"2023-09-28T12:36:23.027972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nnum_comp = 4\n\n# python PCA\npca_out = PCA(n_components=num_comp)\npca_out.fit(train_df_features_norm[columns_to_be_added_as_features])\npca_out_data = pca_out.fit_transform(train_df_features_norm[columns_to_be_added_as_features])\n\nprint(f'explained variance = {pca_out.explained_variance_ratio_} ')\nprint(f'cumulative variance  = {np.cumsum(pca_out.explained_variance_ratio_)}' )\n ","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:23.03006Z","iopub.execute_input":"2023-09-28T12:36:23.030379Z","iopub.status.idle":"2023-09-28T12:36:23.04492Z","shell.execute_reply.started":"2023-09-28T12:36:23.030351Z","shell.execute_reply":"2023-09-28T12:36:23.043991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nloadings = pca_out.components_ \nvar = pca_out.explained_variance_ratio_\npca_cumSum = np.cumsum(var)\n\npc_list = [\"PC\"+str(i) for i in list(range(1, num_comp+1))]\n\n# component loadings or weights (correlation coefficient between original variables and the component) \n# component loadings represents the elements of the eigenvector\n# the squared loadings within the PCs always sums to 1\n\n\nloadings_pca_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))\nloadings_pca_df['variable'] = train_df_features_norm[columns_to_be_added_as_features].columns.values\nloadings_pca_df = loadings_pca_df.set_index('variable')\n\npca_cumSum_df = pd.DataFrame({'var':var,'cumsum_var':pca_cumSum, 'PC':pc_list})\n\n\npca_plot_df = pd.DataFrame({'PC1': pca_out_data[:,0],\n                   'PC2': pca_out_data[:,1],\n                   'PC3': pca_out_data[:,2],\n                   'PC4': pca_out_data[:,3],  \n                   'Survived': train_df_features_norm['Survived']\n                  })","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:23.046434Z","iopub.execute_input":"2023-09-28T12:36:23.046715Z","iopub.status.idle":"2023-09-28T12:36:23.058434Z","shell.execute_reply.started":"2023-09-28T12:36:23.046689Z","shell.execute_reply":"2023-09-28T12:36:23.057712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figA, (ax1, ax2) = plt.subplots(1,2, figsize=(10, 3))\n\n# Variance explained\nsns.barplot(x='PC',y=\"var\", data=pca_cumSum_df, color=\"c\", ax=ax1).set(title='PCA Explained Variance');\nsns.barplot(x='PC',y=\"cumsum_var\", data=pca_cumSum_df, color=\"c\", ax=ax2).set(title='PCA Explained Variance');\n\nsns.pairplot(pca_plot_df, hue=\"Survived\" )\n\n#figB2, (axB1, axB2, axB3) = plt.subplots(1,3, figsize=(10, 4))\n\n#sns.scatterplot(data=pca_plot_df, x='PC1',y=\"PC2\",  hue=\"Survived\" ,ax=axB1).set(title='PCA Plot');\n#sns.scatterplot(data=pca_plot_df, x='PC1',y=\"PC3\",  hue=\"Survived\", ax=axB2).set(title='PCA Plot');\n#sns.scatterplot(data=pca_plot_df, x='PC1',y=\"PC6\",  hue=\"Survived\", ax=axB3).set(title='PCA Plot');","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:23.059609Z","iopub.execute_input":"2023-09-28T12:36:23.059906Z","iopub.status.idle":"2023-09-28T12:36:32.943002Z","shell.execute_reply.started":"2023-09-28T12:36:23.059869Z","shell.execute_reply":"2023-09-28T12:36:32.942201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## 4.0) Model Training\n\nAfter examining the structure of the input data space, we see that it has some\nstructure (e.g., clustered target data) but that it is complex,\ne.g., non-gaussian, nested hierarchies, outliers, non-linearities. \n\nAn SVM, KNN, or decision tree may may get us 70% of the way there but they may run into problems with\nover generalizing or overfitting. Some perturbation, sub-ensemble, or penalties may need to be introducted.  \nPerhaps some more complex feature engineering could be introducted. \n\n* Support Vector Machine \n* Decision Tree\n* K-Nearest Neighbours\n\n(not included yet...)\n- Bagging Decision Tree (Ensemble Learning I)\n- Boosted Decision Tree (Ensemble Learning II)\n- Random Forest (Ensemble Learning III)\n- Naive Bayes\n- Logistic Regression\n- Voting Classification (Ensemble Learning IV)\n- Neural Network (Deep Learning)\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### Splitting Training Data into Test/Validation Sets\nBelow, the validation's ratio was set to 0.2, which means 20% of data will be used to validate it. \nTo split the dataset into training and validation sets, we will be using Sklearn's train_test_split method. \nThen we will be splitting the features and labels as shown in the last 4 lines of the code piece below.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n ## SWAP OUT REG DATA FOR PCA ###\n#train_df_norm = pca_plot_df[['PC1', 'PC2','PC3','PC4', 'Survived']]\n\n#columns_to_be_added_as_features = ['PC1', 'PC2',  'PC3', 'PC4']\n#### SWAP OUT REG DATA FOR PCA  ###\n#train_df_features_norm = pca_plot_df\n    \ntrain_df_features_norm = train_df_features_norm.sample(frac=1).reset_index(drop=True)\n \n\nvalidation_set_ratio = 0.20  # 20\nvalidation_set_size = int(len(train_df_features_norm)*validation_set_ratio)\ntraining_set_size = len(train_df_features_norm) - validation_set_size\n\nprint(\"Total set size: {}\".format(len(train_df_features_norm)))\nprint(\"Training set size: {}\".format(training_set_size))\nprint(\"Validation set size: {}\".format(validation_set_size))\n\n\n# test vs train = 20% split\ntrain, val = train_test_split(train_df_features_norm, test_size=validation_set_ratio)\n\ntrain_X = train[columns_to_be_added_as_features]\ntrain_Y = train['Survived']\n\nval_X = val[columns_to_be_added_as_features]\nval_Y = val['Survived']","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:32.944435Z","iopub.execute_input":"2023-09-28T12:36:32.944989Z","iopub.status.idle":"2023-09-28T12:36:32.957893Z","shell.execute_reply.started":"2023-09-28T12:36:32.944959Z","shell.execute_reply":"2023-09-28T12:36:32.956754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVM Model: training, predictions, and visualizing","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\n\ngrid = {\"C\":np.arange(1,10,1),'gamma':[ 0.00001, 0.00005, 0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1,5]}\nsvm0 = SVC(random_state=42)\nsvm_cv = GridSearchCV(svm0, grid, cv=10)\nsvm_cv.fit(train_X, train_Y.values.ravel())\n\nprint(\"Parameter Selection SVM:\",svm_cv.best_params_)\n\nsvm = SVC(C=svm_cv.best_params_[\"C\"], gamma=svm_cv.best_params_[\"gamma\"],random_state=42)\nsvm.fit(train_X,train_Y)\nprint(\"SVC Accuracy :\",svm.score(val_X,val_Y))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:40:16.852134Z","iopub.execute_input":"2023-09-28T12:40:16.852838Z","iopub.status.idle":"2023-09-28T12:40:33.757201Z","shell.execute_reply.started":"2023-09-28T12:40:16.852802Z","shell.execute_reply":"2023-09-28T12:40:33.755672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_svm = svm.predict(val_X)\n\n\naccuracy = metrics.accuracy_score(val_Y, y_pred_svm)\nprecision = metrics.precision_score(val_Y, y_pred_svm)\nrecall = metrics.recall_score(val_Y, y_pred_svm)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\ncv_score = cross_val_score(svm, train_df_features_norm[columns_to_be_added_as_features], train_df_features_norm['Survived'], cv = 6  )\nprint(\"Cross Val Alt\", cv_score)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:41:01.572497Z","iopub.execute_input":"2023-09-28T12:41:01.572932Z","iopub.status.idle":"2023-09-28T12:41:01.704024Z","shell.execute_reply.started":"2023-09-28T12:41:01.57289Z","shell.execute_reply":"2023-09-28T12:41:01.702687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Model: training, predictions, and visualizing","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\n\n# Decision tree\n\ndecision_tree = tree.DecisionTreeClassifier()\ndecision_tree = decision_tree.fit(train_X, train_Y.values.ravel())\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:36:42.697551Z","iopub.execute_input":"2023-09-28T12:36:42.698256Z","iopub.status.idle":"2023-09-28T12:36:42.706917Z","shell.execute_reply.started":"2023-09-28T12:36:42.698216Z","shell.execute_reply":"2023-09-28T12:36:42.705823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny_pred_tree = decision_tree.predict(val_X)\n\n#print(\"Accuracy:\",metrics.accuracy_score(val_X, y_pred_tree))\n\n\naccuracy = metrics.accuracy_score(val_Y, y_pred_tree)\nprecision = metrics.precision_score(val_Y, y_pred_tree)\nrecall = metrics.recall_score(val_Y, y_pred_tree)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\n\n# Alt Cross Val Method\ncv_score = cross_val_score(decision_tree, train_df_features_norm[columns_to_be_added_as_features], train_df_features_norm['Survived'], cv = 6  )\nprint(\"Cross Val Alt\", cv_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:42:01.24143Z","iopub.execute_input":"2023-09-28T12:42:01.241905Z","iopub.status.idle":"2023-09-28T12:42:01.308801Z","shell.execute_reply.started":"2023-09-28T12:42:01.241869Z","shell.execute_reply":"2023-09-28T12:42:01.306983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forrest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(train_X,train_Y)\nrf.score(val_X,val_Y)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:38:39.643668Z","iopub.execute_input":"2023-09-28T12:38:39.644219Z","iopub.status.idle":"2023-09-28T12:38:40.111693Z","shell.execute_reply.started":"2023-09-28T12:38:39.644176Z","shell.execute_reply":"2023-09-28T12:38:40.109765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### KNN Model: training, predictions, and visualizing\n \n A for loop can be created to check for the optimal value of k \"local neighbourhood\".  \n I found that 3 and 9 worked equally well in a tested range of k= [1:2:15]","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n \nknn0 = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn0, {\"n_neighbors\": np.arange(3,50)}, cv=10)\nknn_cv.fit(train_X,train_Y)\nprint(\"Best parameters of KNN :\",knn_cv.best_params_)\n\nknn = KNeighborsClassifier(n_neighbors=knn_cv.best_params_[\"n_neighbors\"])\nknn.fit(train_X,train_Y.values.ravel())","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:41:31.474186Z","iopub.execute_input":"2023-09-28T12:41:31.474636Z","iopub.status.idle":"2023-09-28T12:41:37.046198Z","shell.execute_reply.started":"2023-09-28T12:41:31.474599Z","shell.execute_reply":"2023-09-28T12:41:37.044936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_knn = knn.predict(val_X)\n\naccuracy =[]\naccuracy.append(metrics.accuracy_score(val_Y, y_pred_knn))\nprecision = metrics.precision_score(val_Y, y_pred_knn)\nrecall = metrics.recall_score(val_Y, y_pred_knn)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\n\nprint(\"KNN Accuracy :\",knn.score(val_X,val_Y))\n\n\n# Alt Cross Val Method\ncv_score = cross_val_score(knn, train_df_features_norm[columns_to_be_added_as_features], train_df_features_norm['Survived'], cv = 5  )\nprint(\"Cross Val Alt\", cv_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:42:08.774728Z","iopub.execute_input":"2023-09-28T12:42:08.775447Z","iopub.status.idle":"2023-09-28T12:42:08.927352Z","shell.execute_reply.started":"2023-09-28T12:42:08.77541Z","shell.execute_reply":"2023-09-28T12:42:08.925954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.0) Winning Model: Selection, Testing, Submission\n\nGenerate predictions for competition test dataset with unknown ground-truth labels. \n\n","metadata":{}},{"cell_type":"code","source":"y_target_predict = knn.predict(test_df_features_norm[columns_to_be_added_as_features]) \n#y_target_predict_svm = svm_model.predict(test_df_features_norm[columns_to_be_added_as_features])  \n\nprint('Predicted result: ', y_target_predict)\nprint(len(y_target_predict))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:42:31.368807Z","iopub.execute_input":"2023-09-28T12:42:31.369422Z","iopub.status.idle":"2023-09-28T12:42:31.424821Z","shell.execute_reply.started":"2023-09-28T12:42:31.369378Z","shell.execute_reply":"2023-09-28T12:42:31.423249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId':test_df_features_Match.PassengerId.values,'Survived':y_target_predict})\nsubmission.Survived = submission.Survived.astype(int)\n\nprint(submission.head()) # make sure we are submitting integers and not floats\nprint(submission.shape)\n\nfilename = 'Titanic Predictions_pub.csv'\nsubmission.to_csv(filename, index=False)\nprint('Saved file: ' + filename)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:42:38.065891Z","iopub.execute_input":"2023-09-28T12:42:38.067689Z","iopub.status.idle":"2023-09-28T12:42:38.097393Z","shell.execute_reply.started":"2023-09-28T12:42:38.067635Z","shell.execute_reply":"2023-09-28T12:42:38.09597Z"},"trusted":true},"execution_count":null,"outputs":[]}]}